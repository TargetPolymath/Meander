{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/Shelves/LiveProjects/Meander/gpt2-tf2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")\n",
    "from scipy import signal\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "!pwd\n",
    "from generate_unconditional_samples import *\n",
    "import importlib\n",
    "import analyze_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_attns = sample_model(nsamples=1, top_k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3666, 40663, 11, 543, 314, 373, 1908, 845, 1036, 45289, 422, 262, 4196, 12017, 11, 318, 1444, 262, 48881, 601, 13, 383, 3047, 329, 852, 281, 33779, 318, 22848, 10809, 11, 290, 1718, 867, 812, 13, 2893, 314, 373, 3047, 11, 314, 4499, 546, 262, 3081, 290, 4202, 286, 9265, 532, 674, 6314, 373, 588, 9397, 13, 314, 973, 616, 40663, 284, 651, 284, 13066, 11, 810, 314, 9713, 262, 3048, 286, 1029, 12, 12287, 20309, 319, 10693, 290, 26697, 13, 383, 4196, 12017, 635, 1838, 6508, 5814, 364, 13]\n",
      "Test\n",
      "ASA Shape: (1, 0, 12, 12, 1, 1024)\n",
      "ASA Shape: (1, 0, 12, 12, 1, 1024)\n",
      "step Tokens: Tensor(\"get_attentions/strided_slice:0\", shape=(1, None), dtype=int32)\n",
      "WARNING:tensorflow:From /home/cydas/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "model Past: None\n",
      "TEST STEP\n",
      "TS 2\n",
      "step Tokens: Tensor(\"strided_slice_1:0\", shape=(1, 1), dtype=int32)\n",
      "model Past: Tensor(\"placeholder:0\", shape=(1, 12, 2, 12, None, 64), dtype=float32)\n",
      "TEST STEP\n",
      "TS 2\n",
      "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n",
      "Sample Data: 1\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(analyze_attention)\n",
    "with open(\"for_analysis.txt\") as fa:\n",
    "    text = fa.read()\n",
    "raw_out_attns = analyze_attention.get_attentions(text, top_k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "batch = 0  # Fixed\n",
    "sample = 0 # Fixed\n",
    "\n",
    "# 0-11 inclusive\n",
    "layer = 11\n",
    "head = 2\n",
    "\n",
    "\n",
    "bias = -0.1\n",
    "\n",
    "# Slice in by batch and ?sample?\n",
    "out_attns = raw_out_attns[batch][sample]\n",
    "num_tokens = len(out_attns)\n",
    "print(num_tokens)\n",
    "# A list of tuples with the decoded token and that token's weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layer | Head | Notes |\n",
    "| ---   | ---  | ---   |\n",
    "| 3  | 0 | Punctuation and structure |\n",
    "| 5  | 0 | Tense? |\n",
    "| 9  | 0 | Relevant Nouns |\n",
    "| 10 | 0 | Relevant Nouns |\n",
    "| 11 | 0 | Punctuation |\n",
    "| 1  | 1 | Word matching?\n",
    "| 1  | 5 | ? |\n",
    "| 1  | 7 | Relevant Nouns |\n",
    "| 1  | 10 | ? |\n",
    "| 2  | 1  | Pretty |\n",
    "| 2  | 7  | ? |\n",
    "| 2  | 8  | Punctuation\n",
    "| 2  | 9  | Relevant Nouns\n",
    "| 2  | 10 | Relevant Nouns\n",
    "| 2  | 11 | ? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_matrix = np.zeros([num_tokens, num_tokens])\n",
    "for i, (token, attention) in enumerate(out_attns):\n",
    "    # print(token)\n",
    "    # print(attention.shape)\n",
    "    attention_matrix[i] = attention[layer, head, sample, 1:num_tokens+1]\n",
    "    # attention_matrix[:, i] += attention[layer, 0]\n",
    "# am_filter = np.array([[-0.25, -0.5, -0.75,   0.5, -0.75,  -0.5, -0.25,     0,     0,     0,     0],\n",
    "#                       [    0, -0.5,    -1,  -1.5,     1,  -1.5,    -1,  -0.5,     0,     0,     0],\n",
    "#                       [    0,    0,    -1,    -2,    -3,     4,    -3,    -2,    -1,     0,     0],\n",
    "#                       [    0,    0,     0,  -0.5,    -1,  -1.5,     1,  -1.5,    -1,  -0.5,     0],\n",
    "#                       [    0,    0,     0,     0, -0.25,  -0.5, -0.75,   0.5, -0.75,  -0.5, -0.25]])\n",
    "attention_matrix[:, 0] = 0\n",
    "attention_matrix += bias\n",
    "\n",
    "# am_filter = np.array([[0.2, 0.6, 0.2]])\n",
    "am_filter = np.array([[1]])\n",
    "f_attention_matrix = signal.convolve2d(attention_matrix, am_filter, mode='same', boundary='fill', fillvalue=-1) - bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971e43d624484cb4843d8daacae2ea78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    fig.clear()\n",
    "except:\n",
    "    pass\n",
    "# fig, (main_ax, side_ax) = plt.subplots(nrows=2)\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "gs = fig.add_gridspec(3, 2)\n",
    "main_ax = fig.add_subplot(gs[0:, 1:])\n",
    "side_ax = fig.add_subplot(gs[1, 0])\n",
    "text_ax = fig.add_subplot(gs[0, 0])\n",
    "text_ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_intuit = 3\n",
    "ignore_center = 2\n",
    "cutoff = 0.05\n",
    "\n",
    "# mouse_text = main_ax.text(-1200, 0, \"MOUSE\", va=\"bottom\", ha=\"left\", color='green')\n",
    "direct_text = text_ax.text(0, 1, \"TEST\", va=\"top\", ha=\"left\", color='blue')\n",
    "# intuit_texts = [text_ax.text(0, 1+0.1*(-2*i-2), f\"CLEVER_{i}\", va=\"top\", ha=\"left\", color=str(float(i)/num_intuit)) for i in range(num_intuit)]\n",
    "intuit_text = text_ax.text(0, 0, \"ASSEMBLED\", color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26:  called the Cypress. The training for being an< astronaut> is grueling, and took many years.\n",
      "Subject ID: 26\n"
     ]
    }
   ],
   "source": [
    "def get_subsample(center_id, envelope=10):\n",
    "    index_range = range(max(0, center_id - envelope), min(center_id + envelope, num_tokens))\n",
    "    tokens = [out_attns[d][0] for d in index_range]\n",
    "    \n",
    "    direct = index_range.index(center_id)\n",
    "    tokens[direct] = f\"<{tokens[direct]}>\"\n",
    "    \n",
    "    to_return = f\"{center_id}: \"+''.join(tokens)\n",
    "    to_return = to_return.replace('\\n', \"|\")\n",
    "    return to_return\n",
    "\n",
    "current_x = 0\n",
    "current_y = 0\n",
    "\n",
    "def onclick(event=None, xdata=None, ydata=None):\n",
    "    if event is None:\n",
    "        assert xdata is not None\n",
    "        assert ydata is not None\n",
    "        main_selector = xdata\n",
    "        \n",
    "    else:\n",
    "        xdata = event.xdata\n",
    "        ydata = event.ydata\n",
    "        main_selector = xdata if event.button == 1 else ydata\n",
    "\n",
    "        \n",
    "    subject_id = int(math.floor(main_selector+0.5))\n",
    "    reference_id = int(math.floor(ydata+0.5))\n",
    "    \n",
    "    current_x = subject_id\n",
    "    current_y = reference_id\n",
    "    \n",
    "    hline.set_ydata(reference_id)\n",
    "    hline2.set_ydata(subject_id)\n",
    "    vline.set_xdata(subject_id)\n",
    "    svline.set_xdata(subject_id)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    direct = get_subsample(subject_id)\n",
    "    print(direct)\n",
    "    direct_text.set_text(direct)\n",
    "    \n",
    "\n",
    "    \n",
    "#     mouse = get_subsample(reference_id)\n",
    "#     print(mouse)\n",
    "#     mouse_text.set_text(mouse)\n",
    "    \n",
    "    # Select Clevers\n",
    "    print(f\"Subject ID: {subject_id}\")\n",
    "    direct_attentions = f_attention_matrix[subject_id]\n",
    "    \n",
    "    # direct_attentions = direct_attentions * (np.abs(np.array(range(-subject_id, num_tokens-subject_id))/100)**0.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dyn_cutoff = np.max(direct_attentions)/2\n",
    "    direct_attentions[direct_attentions<dyn_cutoff] = 0 \n",
    "    # direct_attentions[direct_attentions<cutoff] = 0 \n",
    "    # direct_attentions[0] = 0\n",
    "    direct_attentions[subject_id:] = 0\n",
    "    notation_line.set_ydata((-50*direct_attentions + subject_id))\n",
    "    side_line.set_ydata(direct_attentions)\n",
    "    \n",
    "    # direct_attentions[subject_id-ignore_center:subject_id+ignore_center] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(direct_attentions)\n",
    "\n",
    "    \n",
    "    # ags = np.argsort(direct_attentions)[::-1][ignore_center:num_intuit+ignore_center]\n",
    "    # ags = np.argsort(direct_attentions)[::-1]\n",
    "    ags = np.argwhere(direct_attentions)\n",
    "    # print(f\"Ags: {ags}\")\n",
    "    \n",
    "    constructed_context_str = \"\"\n",
    "    intuit_text.set_text(\"FAIL\")\n",
    "    \n",
    "    last_id = 0\n",
    "    new_id = 0;\n",
    "    for ind in ags:\n",
    "        # print(f\"index: {ind[0]}\")\n",
    "        new_id = int(ind[0])\n",
    "        new_token = out_attns[new_id][0]\n",
    "        # print(new_id, new_token)\n",
    "        if new_id != last_id+1:\n",
    "\n",
    "            if len(constructed_context_str) and constructed_context_str[-1] != ' ':\n",
    "                constructed_context_str += \" \"\n",
    "\n",
    "                \n",
    "            constructed_context_str += \"-\"\n",
    "            if new_token[0] not in [\" \", ',', '.', ';', '?', '!']:\n",
    "                constructed_context_str += \" \"\n",
    "\n",
    "\n",
    "        constructed_context_str += new_token\n",
    "        last_id = new_id\n",
    "\n",
    "    \n",
    "    intuit_text.set_text(constructed_context_str)\n",
    "    \n",
    "#     for ct in intuit_texts:\n",
    "#         ct.set_text(\"\")\n",
    "    \n",
    "#     for clever_text, arg_id in zip(intuit_texts, ags):\n",
    "#         # if direct_attentions[arg_id] > cutoff:\n",
    "#         if True:\n",
    "#             print(arg_id-1)\n",
    "#             ss = get_subsample(arg_id-1)\n",
    "#             print(ss)\n",
    "\n",
    "#             clever_text.set_text(ss)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# fig.canvas.mpl_disconnect(cid)\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "# cid = fig.canvas.mpl_connect('motion_notify_event', onclick)\n",
    "# fig.canvas.mpl_disconnect(cid)\n",
    "\n",
    "\n",
    "# main_ax.matshow(attention_matrix)\n",
    "main_ax.matshow(f_attention_matrix)\n",
    "\n",
    "hline = main_ax.axhline(10)\n",
    "hline2 = main_ax.axhline(10, color='grey')\n",
    "vline = main_ax.axvline(10)\n",
    "\n",
    "svline = side_ax.axvline(10, color='green')\n",
    "\n",
    "demo_index = 26\n",
    "\n",
    "notation_line, = main_ax.plot((-1*f_attention_matrix[demo_index] + 1), color='white')\n",
    "\n",
    "\n",
    "side_line, = side_ax.plot(attention_matrix[demo_index])\n",
    "# side_ax.clear()\n",
    "onclick(xdata=demo_index, ydata=demo_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print([1, 2, 3][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
